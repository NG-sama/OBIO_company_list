{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from typing import Dict, List\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urljoin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EAHNCompanyExtractor:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "        self.base_url = \"https://eahn.obio.ca\"\n",
    "        self.companies_url = f\"{self.base_url}/companies/\"\n",
    "\n",
    "    def fetch_page(self, url: str, retries: int = 3, delay: int = 1) -> str:\n",
    "        \"\"\"Fetch HTML content from URL with retry mechanism.\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = self.session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except requests.RequestException as e:\n",
    "                if attempt == retries - 1:\n",
    "                    raise Exception(f\"Failed to fetch {url} after {retries} attempts: {str(e)}\")\n",
    "                print(f\"Attempt {attempt + 1} failed, retrying...\")\n",
    "                time.sleep(delay)\n",
    "        return \"\"\n",
    "\n",
    "    def extract_main_page_companies(self) -> List[Dict]:\n",
    "        \"\"\"Extract company information from the main companies page.\"\"\"\n",
    "        print(f\"Fetching main company list from {self.companies_url}\")\n",
    "        html_content = self.fetch_page(self.companies_url)\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        companies = []\n",
    "        \n",
    "        # Find all company wrapper divs\n",
    "        company_divs = soup.find_all('div', class_='vc_col-sm-3')\n",
    "        print(f\"Found {len(company_divs)} company entries\")\n",
    "        \n",
    "        for div in company_divs:\n",
    "            company_info = {}\n",
    "            \n",
    "            # Extract company link and logo\n",
    "            company_link = div.find('a')\n",
    "            if company_link:\n",
    "                # Get the full URL\n",
    "                relative_url = company_link.get('href', '')\n",
    "                company_info['url'] = urljoin(self.base_url, relative_url)\n",
    "                \n",
    "                # Extract company name from URL\n",
    "                company_name = re.search(r'/([^/]+)/$', company_info['url'])\n",
    "                if company_name:\n",
    "                    company_info['name'] = company_name.group(1).replace('-', ' ').title()\n",
    "                \n",
    "                # Extract logo URL\n",
    "                logo_img = company_link.find('img')\n",
    "                if logo_img:\n",
    "                    logo_url = logo_img.get('src', '')\n",
    "                    company_info['logo_url'] = urljoin(self.base_url, logo_url)\n",
    "            \n",
    "            if company_info:\n",
    "                companies.append(company_info)\n",
    "        \n",
    "        return companies\n",
    "\n",
    "    def extract_subpage_info(self, html_content: str) -> Dict:\n",
    "        \"\"\"Extract detailed information from company subpage.\"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        company_details = {}\n",
    "        \n",
    "        # Extract company title/heading\n",
    "        title = soup.find('h2', class_='blue')\n",
    "        if title:\n",
    "            company_details['title'] = title.text.strip()\n",
    "        \n",
    "        # Extract main content sections\n",
    "        content_sections = soup.find_all('div', class_='wpb_text_column')\n",
    "        description_parts = []\n",
    "        company_details['points'] = []\n",
    "        \n",
    "        for section in content_sections:\n",
    "            # Extract paragraphs\n",
    "            paragraphs = section.find_all('p')\n",
    "            for p in paragraphs:\n",
    "                text = p.text.strip()\n",
    "                if text:\n",
    "                    description_parts.append(text)\n",
    "                    \n",
    "                    # Look for website link\n",
    "                    links = p.find_all('a')\n",
    "                    for link in links:\n",
    "                        if 'http' in link.get('href', ''):\n",
    "                            company_details['website'] = link.get('href')\n",
    "            \n",
    "            # Extract lists\n",
    "            lists = section.find_all(['ul', 'ol'])\n",
    "            for lst in lists:\n",
    "                for item in lst.find_all('li'):\n",
    "                    company_details['points'].append(item.text.strip())\n",
    "        \n",
    "        company_details['description'] = '\\n'.join(description_parts)\n",
    "        return company_details\n",
    "\n",
    "    def process_companies(self, limit: int = None) -> List[Dict]:\n",
    "        \"\"\"Process companies and fetch their subpage information.\n",
    "        Args:\n",
    "        limit (int, optional): Maximum number of companies to process. If None, process all.\n",
    "    \"\"\"\n",
    "        companies = self.extract_main_page_companies()\n",
    "        if limit:\n",
    "            companies = companies[:limit]\n",
    "        total_companies = len(companies)\n",
    "        \n",
    "        print(f\"\\nStarting to process {total_companies} companies...\")\n",
    "        \n",
    "        for index, company in enumerate(companies, 1):\n",
    "            try:\n",
    "                print(f\"\\nProcessing {company['name']} ({index}/{total_companies})...\")\n",
    "                html_content = self.fetch_page(company['url'])\n",
    "                subpage_info = self.extract_subpage_info(html_content)\n",
    "                company.update(subpage_info)\n",
    "                # Add small delay between requests\n",
    "                time.sleep(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {company['name']}: {str(e)}\")\n",
    "        \n",
    "        return companies\n",
    "\n",
    "def export_to_csv(companies: List[Dict], output_file: str = 'eahn_companies.csv'):\n",
    "    \"\"\"Export company information to CSV file.\"\"\"\n",
    "    import csv\n",
    "    \n",
    "    # Define CSV headers\n",
    "    headers = ['name', 'url', 'logo_url', 'title', 'description', 'website', 'points']\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:  # utf-8-sig for Excel compatibility\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for company in companies:\n",
    "            # Prepare row data\n",
    "            row = {\n",
    "                'name': company.get('name', ''),\n",
    "                'url': company.get('url', ''),\n",
    "                'logo_url': company.get('logo_url', ''),\n",
    "                'title': company.get('title', ''),\n",
    "                'description': company.get('description', ''),\n",
    "                'website': company.get('website', ''),\n",
    "                'points': ';'.join(company.get('points', []))\n",
    "            }\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting EAHN company information extraction (test run with 10 companies)...\n",
      "Fetching main company list from https://eahn.obio.ca/companies/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 110 company entries\n",
      "\n",
      "Starting to process 54 companies...\n",
      "\n",
      "Processing Alethea (1/54)...\n",
      "\n",
      "Processing Bodiometer (2/54)...\n",
      "\n",
      "Processing Nanotess (3/54)...\n",
      "\n",
      "Processing Rostrum Medical Innovations (4/54)...\n",
      "\n",
      "Processing Tochtech Technologies (5/54)...\n",
      "\n",
      "Processing Zamplo (6/54)...\n",
      "\n",
      "Processing A4I (7/54)...\n",
      "\n",
      "Processing Able Innovations (8/54)...\n",
      "\n",
      "Processing Ai Vali (9/54)...\n",
      "\n",
      "Processing Amacathera (10/54)...\n",
      "\n",
      "Processing Awake Labs (11/54)...\n",
      "\n",
      "Processing Braze Mobility (12/54)...\n",
      "\n",
      "Processing Cosm Medical (13/54)...\n",
      "\n",
      "Processing Eapoc Evidence At The Point Of Care (14/54)...\n",
      "\n",
      "Processing Enhanced Medical Nutrition (15/54)...\n",
      "\n",
      "Processing Flosonics Medical (16/54)...\n",
      "\n",
      "Processing Fluidai Medical (17/54)...\n",
      "\n",
      "Processing Focal Healthcare (18/54)...\n",
      "\n",
      "Processing Frontline (19/54)...\n",
      "\n",
      "Processing Goji Technology Systems (20/54)...\n",
      "\n",
      "Processing Gotcare (21/54)...\n",
      "\n",
      "Processing Huron Digital Pathology (22/54)...\n",
      "\n",
      "Processing Hyivy Health (23/54)...\n",
      "\n",
      "Processing Hypercare (24/54)...\n",
      "\n",
      "Processing Inventorr Md (25/54)...\n",
      "\n",
      "Processing Ironstone Product Development (26/54)...\n",
      "\n",
      "Processing Ka Imaging (27/54)...\n",
      "\n",
      "Processing Llif Healthcare (28/54)...\n",
      "\n",
      "Processing M Health Solutions (29/54)...\n",
      "\n",
      "Processing Managinglife (30/54)...\n",
      "\n",
      "Processing Memotext (31/54)...\n",
      "\n",
      "Processing Mobiointeractive (32/54)...\n",
      "\n",
      "Processing Neurovine (33/54)...\n",
      "\n",
      "Processing Nextup Care (34/54)...\n",
      "\n",
      "Processing Oncoustics (35/54)...\n",
      "\n",
      "Processing Phyxable (36/54)...\n",
      "\n",
      "Processing Swift Medical (37/54)...\n",
      "\n",
      "Processing Synaptive (38/54)...\n",
      "\n",
      "Processing Tamvoes (39/54)...\n",
      "\n",
      "Processing Televu (40/54)...\n",
      "\n",
      "Processing Therapeutic Monitoring Systems (41/54)...\n",
      "\n",
      "Processing Toefx (42/54)...\n",
      "\n",
      "Processing Trexo Robotics (43/54)...\n",
      "\n",
      "Processing Vena Medical (44/54)...\n",
      "\n",
      "Processing Voycecanada (45/54)...\n",
      "\n",
      "Processing Braver (46/54)...\n",
      "\n",
      "Processing Breathesuite (47/54)...\n",
      "\n",
      "Processing Grayos (48/54)...\n",
      "\n",
      "Processing My01 (49/54)...\n",
      "\n",
      "Processing Perceiv Ai (50/54)...\n",
      "\n",
      "Processing Polyunity (51/54)...\n",
      "\n",
      "Processing Pragmaclin (52/54)...\n",
      "\n",
      "Processing Alethea (53/54)...\n",
      "\n",
      "Processing Bodiometer (54/54)...\n",
      "\n",
      "Successfully exported 54 companies to eahn_companies.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        print(\"Starting EAHN company information extraction (test run with 10 companies)...\")\n",
    "        extractor = EAHNCompanyExtractor()\n",
    "        companies = extractor.process_companies(limit=54)\n",
    "        \n",
    "        # Export to CSV\n",
    "        output_file = 'eahn_companies.csv'\n",
    "        export_to_csv(companies, output_file)\n",
    "        print(f\"\\nSuccessfully exported {len(companies)} companies to {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
